{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMBNCcfi3bR29p+DjNwkb77",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rcdbe/sma-online/blob/master/day-2/python/001_Text_Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iipKPhvLB3Pr",
        "colab_type": "text"
      },
      "source": [
        "#TEXT CLEANING (ENGLISH)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKxwNLA-4yUw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Menginstall package pandas\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_colwidth', -1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "potGRan16U7J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Memilih data text untuk melalui proses cleaning\n",
        "train_data = pd.read_csv('https://raw.githubusercontent.com/rcdbe/sma-online/master/day-2/source/train.csv')\n",
        "train_data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBM1DZOR6b8O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Membuat fungsi Text Cleaning\n",
        "import re\n",
        "def  clean_text(df, text_field, new_text_field_name):\n",
        "    df[new_text_field_name] = df[text_field].str.lower()\n",
        "    df[new_text_field_name] = df[new_text_field_name].apply(lambda elem: re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", elem))  \n",
        "    # remove numbers\n",
        "    df[new_text_field_name] = df[new_text_field_name].apply(lambda elem: re.sub(r\"\\d+\", \"\", elem))\n",
        "    return df\n",
        "    \n",
        "data_clean = clean_text(train_data, 'text', 'text_clean')\n",
        "data_clean.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mapgNr0l6kee",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Mengimport corpus stopwords dari library Nltk\n",
        "import nltk.corpus\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVbjBp1W6qvq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Membersihkan 'stopwords' yang terdapat didalam teks\n",
        "from nltk.corpus import stopwords\n",
        "stop = stopwords.words('english')\n",
        "\n",
        "data_clean['text_clean'] = data_clean['text_clean'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
        "data_clean.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhF_Glzv6xIt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Mengimport Parameter punkt\n",
        "import nltk \n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrkxYqIL7Ebl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Melakukan proses tokenization\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "data_clean['text_tokens'] = data_clean['text_clean'].apply(lambda x: word_tokenize(x))\n",
        "data_clean.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tx5GMOfo7JtV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Mengimport library yang dibutuhkan untuk proses Stemming\n",
        "from nltk.stem import PorterStemmer \n",
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Otx2dKbB7OPS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Membuat Fungsi Stemmer\n",
        "def word_stemmer(text):\n",
        "    stem_text = [PorterStemmer().stem(i) for i in text]\n",
        "    return stem_text\n",
        "    \n",
        "data_clean['text_tokens_stem'] = data_clean['text_tokens'].apply(lambda x: word_stemmer(x))\n",
        "data_clean.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WEZvs4T7Vym",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Mengimport library yang dibutuhkan untuk proses lemmatization\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rGYrO_l7aAc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Membuat Fungsi Lemmatization\n",
        "def word_lemmatizer(text):\n",
        "    lem_text = [WordNetLemmatizer().lemmatize(i, pos='n') for i in text]\n",
        "    return lem_text\n",
        "    \n",
        "data_clean['text_tokens_lemma'] = data_clean['text_tokens'].apply(lambda x: word_lemmatizer(x))\n",
        "data_clean.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "goCUBqBFNxnA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Menyimpan data\n",
        "data_clean.to_csv('saved.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSxcjeBlD-9q",
        "colab_type": "text"
      },
      "source": [
        "#TEXT CLEANING (INDONESIA)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOZwOhXmEEAj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Memilih data text untuk melalui proses cleaning\n",
        "grab_data = pd.read_csv('https://raw.githubusercontent.com/rcdbe/sma-online/master/day-2/source/tweet-indo.csv')\n",
        "grab_data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKfxYVpqGdzl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Membuat fungsi Text Cleaning\n",
        "import re\n",
        "def  clean_text(df, text_field, new_text_field_name):\n",
        "    df[new_text_field_name] = df[text_field].str.lower()\n",
        "    df[new_text_field_name] = df[new_text_field_name].apply(lambda elem: re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", elem))  \n",
        "    # remove numbers\n",
        "    df[new_text_field_name] = df[new_text_field_name].apply(lambda elem: re.sub(r\"\\d+\", \"\", elem))\n",
        "    \n",
        "    return df\n",
        "data_clean = clean_text(grab_data, 'text', 'text_clean')\n",
        "data_clean.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GxmaBMNGwuH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Mengimport corpus stopwords dari library Nltk\n",
        "import nltk.corpus\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbl_tVK9HANR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Membersihkan 'stopwords' yang terdapat didalam teks\n",
        "from nltk.corpus import stopwords\n",
        "stop = stopwords.words('indonesian')\n",
        "\n",
        "data_clean['text_clean'] = data_clean['text_clean'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
        "data_clean.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "601O77OtIeO2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Mengimport Parameter punkt\n",
        "import nltk \n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0wAd9T8NSIG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Melakukan proses tokenization\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "data_clean['text_tokens'] = data_clean['text_clean'].apply(lambda x: word_tokenize(x))\n",
        "data_clean.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jVdR5isNuYX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Menginstall package sastrawi untuk stemming\n",
        "!pip install sastrawi\n",
        "import re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2t3ITyYPLzR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Membuat fungsi Stemming\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "def word_stemmer(text):\n",
        "    stem_text = [StemmerFactory().create_stemmer().stem(i) for i in text]\n",
        "    return stem_text\n",
        "    \n",
        "data_clean['text_tokens_stem'] = data_clean['text_tokens'].apply(lambda x: word_stemmer(x))\n",
        "data_clean.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OAeM0ygwQZFg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Menyimpan data\n",
        "data_clean.to_csv('cleaningindonesian.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}